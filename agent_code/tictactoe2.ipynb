{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, numpy as np, matplotlib.pyplot as plt\n",
    "import tttenv\n",
    "\n",
    "BOARD_ROWS,BOARD_COLS = 3,3\n",
    "TOTAL_BOARD_SPACES = BOARD_ROWS*BOARD_COLS\n",
    "COORD_TO_INDEX = lambda x : (x[0] * BOARD_ROWS) + x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from datetime import datetime\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())\n",
    "currenttime = datetime.now()\n",
    "timetext = str(currenttime.year) + '-' + str(currenttime.month) + '-' + str(currenttime.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super class for players (both human and agent)\n",
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        pass\n",
    "        \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that defines agent players\n",
    "# Two of these players will be learning and playing with each other\n",
    "class QAgentPlayer:\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        # Implement epsilon-greedy method of selecting actions\n",
    "        # Default .3 value means 30% of time agent takes random action, 70% of time agent takes greedy action\n",
    "        self.explRate = explRate\n",
    "        # Record all positions taken\n",
    "        self.states = []\n",
    "        # Learning rate \n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        # State -> Value\n",
    "        self.statesValue = {}\n",
    "        \n",
    "    # Get a unique hash value that corresponds with the given board state\n",
    "    def getHash(self, board):\n",
    "        return str(board.reshape(BOARD_ROWS * BOARD_COLS))\n",
    "    \n",
    "    # Using this abstraction because HumanPlayer class will have this as well\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard):\n",
    "        if np.random.uniform(0,1) <= self.explRate:\n",
    "            # Take random action\n",
    "            index = np.random.choice(len(openPositions))\n",
    "            action = openPositions[index]\n",
    "        else:\n",
    "            maxValue = -999\n",
    "            for p in openPositions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                nextBoard[p] = self.turn\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    # At the end of the game, backpropogate and update state values\n",
    "    # The updated value of state t equals the current value of state t\n",
    "    #   adding the difference between the value of next state and the value of current state,\n",
    "    #   which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0)\n",
    "    def feedReward(self, reward):\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    # For when there's a new round\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    # After training, an agent has its policy stored in self.stateValues\n",
    "    # This can be saved to play against a human player\n",
    "    def savePolicy(self):\n",
    "        with open(currentdir + '/policies/ttt_policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.statesValue, fw)\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.statesValue = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that inherits most of QAgent player\n",
    "# Implements deep learning instead of Q-learning\n",
    "class DeepAgentPlayer(QAgentPlayer):\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        super(DeepAgentPlayer, self, name, turn, explRate).__init__()\n",
    "        \n",
    "    def loadModel(self):\n",
    "        s = 'model_values' + self.tag + '.h5'\n",
    "        model_file = Path(s)\n",
    "        if model_file.is_file():\n",
    "            model = Km.load_model(s)\n",
    "        else:\n",
    "            model = Km.Sequential()\n",
    "            model.add(Kl.Dense(18, activation='relu', input_dim=9))\n",
    "            model.add(Kl.Dense(18, activation='relu'))\n",
    "            model.add(Kl.Dense(1, activation='linear'))\n",
    "            model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for human player\n",
    "# Mostly inherited from super class Player\n",
    "class HumanPlayer(Player):    \n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        while True:\n",
    "            try:\n",
    "                i = int(input(\"Input action row-> \"))\n",
    "                j = int(input(\"Input action column-> \"))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if (i, j) in positions:\n",
    "                return (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find vacant positions after a turn is made\n",
    "def availablePositions(board):\n",
    "    positions = []\n",
    "    for i in range(BOARD_ROWS):\n",
    "        for j in range(BOARD_COLS):\n",
    "            if board[i,j] == 0:\n",
    "                # Coordinates need to be in tuple form\n",
    "                positions.append((i,j))\n",
    "    return positions\n",
    "\n",
    "def startGame(p1, p2, env):\n",
    "    gameOver = False\n",
    "    human = p1.name == \"human\" or p2.name == \"human\"\n",
    "    observation = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "    while not gameOver:\n",
    "        # Player 1\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p1.chooseAction(openPositions, observation))\n",
    "        p1.addState(actionHash)\n",
    "        # When there's a human player, print out stuff\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            elif human:\n",
    "                print(\"tie!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        # Player 2\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p2.chooseAction(openPositions, observation))\n",
    "        p2.addState(actionHash)\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()\n",
    "    return 1 if reward[0] == 1 else (2 if reward[1] == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training and testing for 2 games...\n",
      "finished training and testing for 4 games...\n",
      "finished training and testing for 8 games...\n",
      "finished training and testing for 16 games...\n",
      "finished training and testing for 32 games...\n",
      "finished training and testing for 64 games...\n",
      "finished training and testing for 128 games...\n",
      "finished training and testing for 256 games...\n",
      "finished training and testing for 512 games...\n",
      "finished training and testing for 1024 games...\n",
      "finished training and testing for 2048 games...\n",
      "finished training and testing for 4096 games...\n",
      "finished training and testing for 8192 games...\n",
      "finished training and testing for 16384 games...\n",
      "finished training and testing for 32768 games...\n",
      "finished training and testing for 65536 games...\n",
      "finished training and testing for 131072 games...\n",
      "finished training and testing for 262144 games...\n",
      "saving results in file graph_2020-3-17 ...\n"
     ]
    }
   ],
   "source": [
    "# Analysis of agent performance\n",
    "env = gym.make('tttenv-v0')\n",
    "p1Results = [[], []]\n",
    "p2Results = [[], []]\n",
    "ties = [[], []]\n",
    "count = 0\n",
    "for exp in range(1, 19):\n",
    "    print('...', end='\\r')\n",
    "    i = 2 ** exp\n",
    "    p1 = QAgentPlayer(\"p1\", 1)\n",
    "    p2 = QAgentPlayer(\"p2\", -1)\n",
    "    for _ in range(i):\n",
    "        startGame(p1, p2, env)\n",
    "    # Test agents\n",
    "    p1.explRate = 0\n",
    "    p2.explRate = 0\n",
    "    # x-axis is games played\n",
    "    # y-axis is games won/tied\n",
    "    p1Results[0].append(i)\n",
    "    p1Results[1].append(0)\n",
    "    p2Results[0].append(i)\n",
    "    p2Results[1].append(0)\n",
    "    ties[0].append(i)\n",
    "    ties[1].append(0)\n",
    "    for _ in range(100):\n",
    "        result = startGame(p1, p2, env)\n",
    "        if result == 0:\n",
    "            ties[1][count] += 1\n",
    "        elif result == 1:\n",
    "            p1Results[1][count] += 1\n",
    "        elif result == 2:\n",
    "            p2Results[1][count] += 1\n",
    "    count += 1\n",
    "    print(f'finished training and testing for {i} games...')\n",
    "print(f'saving results in file graph_{timetext} ...')\n",
    "# Save results\n",
    "with open(currentdir + '/graphingData/graph_' + timetext, 'wb') as f:\n",
    "    pickle.dump((p1Results, p2Results, ties), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph results\n",
    "plt.semilogx(p1Results[0], p1Results[1], label='Player 1', basex=2)\n",
    "plt.semilogx(p2Results[0], p2Results[1], label='Player 2', basex=2)\n",
    "plt.semilogx(ties[0], ties[1], label='Ties', basex=2)\n",
    "plt.xlabel('Games Played For Training') \n",
    "plt.ylabel('Games Favoring Result') \n",
    "plt.title('Tic Tac Toe') \n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "saved p1 policy\n",
      "saved p2 policy\n"
     ]
    }
   ],
   "source": [
    "# Train agents\n",
    "p1 = QAgentPlayer(\"p1\", 1)\n",
    "p2 = QAgentPlayer(\"p2\", -1)\n",
    "env = gym.make('tttenv-v0')\n",
    "print(\"training...\")\n",
    "for _ in range(1000):\n",
    "    startGame(p1, p2, env)\n",
    "# Save Results\n",
    "p1.savePolicy()\n",
    "print(\"saved p1 policy\")\n",
    "p2.savePolicy()\n",
    "print(\"saved p2 policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human play with trained p1\n",
    "# Make sure Agent isn't training anymore\n",
    "p1 = QAgentPlayer(\"computer\", 1, explRate=0)\n",
    "p1.loadPolicy(currentdir + \"/policies/ttt_policy_p1\")\n",
    "p2 = HumanPlayer(\"human\")\n",
    "startGame(p1, p2, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human play with trained p2\n",
    "# Make sure Agent isn't training anymore\n",
    "p1 = HumanPlayer(\"human\")\n",
    "p2 = QAgentPlayer(\"computer\", -1, explRate=0)\n",
    "p2.loadPolicy(currentdir + \"/policies/ttt_policy_p2\")\n",
    "startGame(p1, p2, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
