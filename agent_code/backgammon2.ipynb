{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, numpy as np,random,copy,matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import count\n",
    "from gym_backgammon.envs.backgammon import WHITE, BLACK, COLORS, TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_backgammon:backgammon-v0')\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class\n",
    "\n",
    "Default super class that lays out defualt functions for both human and agent players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Default player class.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the player.\n",
    "    '''\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "        self.name = 'AgentExample({})'.format(self.color)\n",
    "        \n",
    "    def chooseAction(self, actions, currentBoard, dice, env):\n",
    "        pass\n",
    "   \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning\n",
    "\n",
    "Agents of this class use a QLearning table to keep track of rewards.  Two of these agents will play against each other during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentPlayer:\n",
    "    '''\n",
    "    Agent player that learns from QLearning table.\n",
    "\n",
    "    Attributes:\n",
    "        name (str) : Name of Player.\n",
    "        turn (int) : Number that designates their turn.\n",
    "        explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        states (Python list) : All positions taken in current game\n",
    "        lr (float) : Learning Rate, used when feeding reward.\n",
    "        decayGamma (float) : Used when feeding reward.\n",
    "        statesValue (Python dict) : QLearning table. Stoes total reward for each possible game state.\n",
    "    '''\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        '''\n",
    "        Constructor for QAgentPlayer class.\n",
    "\n",
    "        Parameters:\n",
    "            name (str) : Name of Player.\n",
    "            turn (int) : Number that designates their turn.\n",
    "            explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        self.explRate = explRate\n",
    "        self.states = []\n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        self.statesValue = {}\n",
    "        self.color = \"\"\n",
    "\n",
    "    \n",
    "    def roll_dice(self):\n",
    "        return (-random.randint(1, 6), -random.randint(1, 6)) if self.color == WHITE else (random.randint(1, 6), random.randint(1, 6))\n",
    "\n",
    "     \n",
    "        \n",
    "    def getHash(self, board):\n",
    "        '''\n",
    "        Get a unique hash value that corresponds with the given board state.\n",
    "        \n",
    "        Parameters:\n",
    "            board (np.ndarray) : Current board (from environment).\n",
    "        \n",
    "        Returns:\n",
    "            str: Hash value of board.\n",
    "        '''\n",
    "        return str(board.flatten)    \n",
    "\n",
    "    def addState(self, state):\n",
    "        '''\n",
    "        Adds a state to the Players state attribute\n",
    "        \n",
    "        Parameters:\n",
    "            state (str) : Put state's hash value into self.states after choosing action.\n",
    "        '''\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, actions, currentBoard, env, roll):\n",
    "        '''\n",
    "        Choose an action to take using epsilon-greedy method.\n",
    "        \n",
    "        Parameters:\n",
    "            openPositions (Python list) : List of available places to make a move on board.\n",
    "            currentBoard (np.ndarray) : Game board.\n",
    "        \n",
    "        Returns:\n",
    "            int: Position of board that agent wants to make a move.\n",
    "        '''\n",
    "        if not actions:\n",
    "            # No moves can be made\n",
    "            return []\n",
    "        \n",
    "        # Possible random action\n",
    "        index = np.random.choice(len(actions))\n",
    "        action = openPositions[index]\n",
    "        secondAction = action\n",
    "        maxValue = -999\n",
    "                    \n",
    "        if np.random.uniform(0,1) > self.explRate:\n",
    "            secondMax = -999\n",
    "            for p in actions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                for move in p:\n",
    "                    nextBoard = env.updateBoard(move,nextBoard)\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    secondMax = maxValue\n",
    "                    secondAction = action\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        if maxValue <= 0:\n",
    "            # If agent can't find a best move, pick a random one anyways\n",
    "            index = np.random.choice(len(actions))\n",
    "            action = actions[index]\n",
    "            secondAction = action\n",
    "        # 20% of the time actually choose the second best action, for variance in decision making\n",
    "        return action if np.random.uniform(0,1) > 0.2 else secondAction\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        '''\n",
    "        At the end of the game, backpropogate and update state values.\n",
    "        The updated value of state t equals the current value of state t\n",
    "            adding the difference between the value of next state and the value of current state,\n",
    "            which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0).\n",
    "        \n",
    "        Parameters:\n",
    "            reward (float) : The reward determined by the environment.\n",
    "        '''\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset board when agent starts a new game.\n",
    "        '''\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        '''\n",
    "        After training, an agent has its policy stored in self.stateValues.\n",
    "        This function saves that attribute in a file to play later.\n",
    "        '''\n",
    "        with open(currentdir + '/policies/c4_policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.statesValue, fw)\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        '''\n",
    "        Reload previous self.stateValues.\n",
    "        \n",
    "        Parameters:\n",
    "            file (str) : Name of file that has policy.\n",
    "        '''\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.statesValue = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player\n",
    "\n",
    "Adds functionality for human player using raw input on the cmdline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    '''\n",
    "    Allows user to input moves through the cmdline.\n",
    "    \n",
    "    Parameters:\n",
    "        openPositions (Python list) : List of available places to make a move on board.\n",
    "        currentBoard (np.ndarray) : Game board.\n",
    "    \n",
    "    Returns:\n",
    "        int: Position of board that user wants to make a move.\n",
    "    '''\n",
    "    def chooseAction(self, positions, currentBoard, env, roll):\n",
    "        message = f\"You rolled a {roll[0]} and {roll[1]}, select where you want to move checkers to and from\\n\"\n",
    "        while True:\n",
    "            try:\n",
    "                i = input(message+'First move (0 for bar, 25 for white home, 26 for black home: ')\n",
    "                i = tuple([int(x) for x in i.split(',')])\n",
    "                j = input(message+'Second move (0 for bar, 25 for white home, 26 for black home: ')\n",
    "                j = tuple([int(x) for x in j.split(',')])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if len(i) != 2 or len(j) != 2:\n",
    "                continue\n",
    "            elif (i,j) in positions:\n",
    "                return (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code\n",
    "\n",
    "The code below is how a game is started, Using 2 player objects and an instance of the tic tac toe environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #  def roll_dice():\n",
    " #       return (-random.randint(1, 6), -random.randint(1, 6)) if self.color == WHITE else (random.randint(1, 6), random.randint(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startGame(pwhite, pblack, env):\n",
    "    '''\n",
    "    Initiates a game of Backgammon.\n",
    "\n",
    "    Parameters:\n",
    "        white (Player) : Player who uses white checkers.\n",
    "        black (Player) : Player who uses black checkers.\n",
    "        env (BackgammonEnv) : Environment for the game.        \n",
    "    ''' \n",
    "    gameOver = False\n",
    "    human = pwhite.name == \"human\" or pblack.name == \"human\"\n",
    "    # When there's a human player, print out stuff\n",
    "    if human:\n",
    "        env.render()\n",
    "\n",
    "    # Determine who goes first\n",
    "    roll = pwhite.roll_dice()\n",
    "    while roll[0] == roll[1]:\n",
    "        # Reroll if they're equal\n",
    "        roll = pblack.roll_dice()\n",
    "\n",
    "    if roll[0] > roll[1]:\n",
    "        if human:\n",
    "            print('White goes first!')\n",
    "        env.playerTurn = WHITE\n",
    "        p1 = pwhite\n",
    "        p2 = pblack\n",
    "    else:\n",
    "        if human:\n",
    "            print('Black goes first!')\n",
    "        env.playerTurn = BLACK\n",
    "        p1 = pblack\n",
    "        p2 = pwhite\n",
    "\n",
    "    while not gameOver:\n",
    "        # Player who goes first\n",
    "        openPositions = env.get_valid_actions(roll)\n",
    "        # Example: [((4,6),(3,7)),((4,8),(2,4))]\n",
    "        #   In this example the dice roll came up 2 and 4. The first choice is to move a checker from spot 4 on the board to spot 6, and then move a checker from 3 to 7\n",
    "        #   The second choice is to move a checker from 4 to 8 and then 2 to 4\n",
    "        all_moves_in_turn = p1.chooseAction(openPositions, env.board, env, roll) # This will return a tuple with all the actions for the player's turn\n",
    "        for move in all_moves_in_turn:\n",
    "            # The for loop then loops through the tuple and invokes each action on the board\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p1.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = p1.roll_dice()\n",
    "\n",
    "        # Player who goes second\n",
    "        openPositions = env.get_valid_actions(roll)\n",
    "        all_moves_in_turn = p2.chooseAction(openPositions, env.board, env, roll)\n",
    "        for move in all_moves_in_turn:\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p2.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = p2.roll_dice()\n",
    "\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()\n",
    "    return 1 if reward[0] == 1 else (2 if reward[1] == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('backgammon-v0') #make env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agents,numGames=2000,explRate=.3):\n",
    "    env = gym.make('backgammon-v0') #make env\n",
    "    for agent in agents:\n",
    "        agent.lr = .2\n",
    "        agent.explRate = explRate\n",
    "    for i in range(numGames):\n",
    "        random.shuffle(agents)\n",
    "        agents[0].turn,agents[1].turn = 1,-1\n",
    "        agents[0].color = WHITE\n",
    "        agents[1].color = BLACK\n",
    "        startGame(agents[0],agents[1],env)\n",
    "        agents[2].turn,agents[3].turn = 1,-1\n",
    "        agents[2].color = WHITE\n",
    "        agents[3].color = BLACK        \n",
    "        startGame(agents[2],agents[3],env)\n",
    "        agents[4].turn,agents[5].turn = 1,-1\n",
    "        agents[4].color = WHITE\n",
    "        agents[5].color = BLACK\n",
    "        startGame(agents[4],agents[5],env)\n",
    "\n",
    "def test(agents,numGames=2000):\n",
    "    for agent in agents:\n",
    "        agent.lr = 0\n",
    "        agent.explRate = .05\n",
    "\n",
    "    graphStats = {name:[0,0,0] for name in ['p1','p2','p3','p4','p5','p6']}\n",
    "\n",
    "    for i in range(numGames):\n",
    "        random.shuffle(agents)\n",
    "        agents[0].turn,agents[1].turn = 1,-1\n",
    "        agents[0].color = WHITE\n",
    "        agents[1].color = BLACK\n",
    "        result = startGame(agents[0],agents[1],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[0].name][2] += 1\n",
    "            graphStats[agents[1].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[0].name][0] += 1\n",
    "            graphStats[agents[1].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[0].name][1] += 1\n",
    "            graphStats[agents[1].name][0] += 1\n",
    "        agents[2].turn,agents[3].turn = 1,-1\n",
    "        agents[2].color = WHITE\n",
    "        agents[3].color = BLACK\n",
    "        result = startGame(agents[2],agents[3],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[2].name][2] += 1\n",
    "            graphStats[agents[3].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[2].name][0] += 1\n",
    "            graphStats[agents[3].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[2].name][1] += 1\n",
    "            graphStats[agents[3].name][0] += 1\n",
    "        agents[4].turn,agents[5].turn = 1,-1\n",
    "        agents[4].color = WHITE\n",
    "        agents[5].color = BLACK\n",
    "        result = startGame(agents[4],agents[5],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[4].name][2] += 1\n",
    "            graphStats[agents[5].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[4].name][0] += 1\n",
    "            graphStats[agents[5].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[4].name][1] += 1\n",
    "            graphStats[agents[5].name][0] += 1\n",
    "    return graphStats\n",
    "\n",
    "def graphResults(graphStats):\n",
    "    fig,axes = plt.subplots(2,3, figsize=(15,15))\n",
    "    xname = ['wins','losses','ties']\n",
    "    count = 0\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            agent = agents[count]\n",
    "            ax = axes[i,j]\n",
    "            ax.bar(xname,graphStats[agent.name])\n",
    "            ax.set_xticklabels(xname)\n",
    "            ax.set_title(f'{agent.name}\\'s Results')\n",
    "            count += 1\n",
    "    plt.show()\n",
    "\n",
    "def createNewGeneration(agents,graphStats):\n",
    "    maxScore = 0\n",
    "    for agent in agents:\n",
    "        agentScore = graphStats[agent.name][0] + (graphStats[agent.name][2] * .5)\n",
    "        if agentScore > maxScore:\n",
    "            # Agent with best score (accounting for wins and ties) will be the basis for the next generation\n",
    "            survivor = agent\n",
    "            maxScore = agentScore\n",
    "\n",
    "    np1 = copy.deepcopy(survivor)\n",
    "    np1.name = 'p1'\n",
    "    np2 = copy.deepcopy(survivor)\n",
    "    np2.name = 'p2'\n",
    "    np3 = copy.deepcopy(survivor)\n",
    "    np3.name = 'p3'\n",
    "    np4 = copy.deepcopy(survivor)\n",
    "    np4.name = 'p4'\n",
    "    np5 = copy.deepcopy(survivor)\n",
    "    np5.name = 'p5'\n",
    "    np6 = copy.deepcopy(survivor)\n",
    "    np6.name ='p6'\n",
    "    return [np1,np2,np3,np4,np5,np6]\n",
    "\n",
    "def savePolicy(agents):\n",
    "    for agent in agents:\n",
    "        agent.savePolicy()\n",
    "        print('saved an agent:',currentdir + '/policies/ttt_policy_' + str(agent.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-45b76a8ae9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexplore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumGames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mexplore\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgraphStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-d71b9dac5493>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agents, numGames, explRate)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWHITE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mstartGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWHITE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-4f8240a2e8fe>\u001b[0m in \u001b[0;36mstartGame\u001b[0;34m(pwhite, pblack, env)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgameOver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Player who goes first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mopenPositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Example: [((4,6),(3,7)),((4,8),(2,4))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#   In this example the dice roll came up 2 and 4. The first choice is to move a checker from spot 4 on the board to spot 6, and then move a checker from 3 to 7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CodingPractice/srProj/senior-project-MARL/gym-backgammon/gym_backgammon/envs/backgammon_env.py\u001b[0m in \u001b[0;36mget_valid_actions\u001b[0;34m(self, roll)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_valid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid_plays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_opponent_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CodingPractice/srProj/senior-project-MARL/gym-backgammon/gym_backgammon/envs/backgammon.py\u001b[0m in \u001b[0;36mget_valid_plays\u001b[0;34m(self, player, roll)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0mroll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1394\u001b[0m             \u001b[0mbar_plays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bar_plays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bar_plays_double\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "p1 = QAgentPlayer(\"p1\", 1)\n",
    "p2 = QAgentPlayer(\"p2\", -1)\n",
    "p3 = QAgentPlayer(\"p3\", 1)\n",
    "p4 = QAgentPlayer(\"p4\", -1)\n",
    "p5 = QAgentPlayer(\"p5\", 1)\n",
    "p6 = QAgentPlayer(\"p6\", -1)\n",
    "agents = [p1,p2,p3,p4,p5,p6]\n",
    "\n",
    "epochs = 10\n",
    "explore = .8\n",
    "for i in range(epochs):\n",
    "    train(agents,numGames=2000,explRate=explore)\n",
    "    explore -= .05\n",
    "    graphStats = test(agents)\n",
    "    agents = createNewGeneration(agents,graphStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
