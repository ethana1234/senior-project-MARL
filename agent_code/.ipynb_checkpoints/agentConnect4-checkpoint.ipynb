{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, numpy as np\n",
    "import connect4env\n",
    "\n",
    "BOARD_ROWS,BOARD_COLS = 6,7\n",
    "TOTAL_BOARD_SPACES = BOARD_ROWS*BOARD_COLS\n",
    "COORD_TO_INDEX = lambda x : (x[0] * BOARD_ROWS) + x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super class for players (both human and agent)\n",
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        pass\n",
    "        \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that defines agent players\n",
    "# Two of these players will be learning and playing with each other\n",
    "class AgentPlayer:\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        # Implement epsilon-greedy method of selecting actions\n",
    "        # Default .3 value means 30% of time agent takes random action, 70% of time agent takes greedy action\n",
    "        self.explRate = explRate\n",
    "        # Record all positions taken\n",
    "        self.states = []\n",
    "        # Learning rate \n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        # State -> Value\n",
    "        self.statesValue = {}\n",
    "        \n",
    "    # Get a unique hash value that corresponds with the given board state\n",
    "    def getHash(self, board):\n",
    "        return str(board.reshape(BOARD_ROWS * BOARD_COLS))\n",
    "    \n",
    "    # Using this abstraction because HumanPlayer class will have this as well\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard):\n",
    "        if np.random.uniform(0,1) <= self.explRate:\n",
    "            # Take random action\n",
    "            index = np.random.choice(len(openPositions))\n",
    "            action = openPositions[index]\n",
    "        else:\n",
    "            maxValue = -999\n",
    "            action = openPositions[0]\n",
    "            for p in openPositions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                nextBoard[p[0]][p[1]] = self.turn\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    # At the end of the game, backpropogate and update state values\n",
    "    # The updated value of state t equals the current value of state t\n",
    "    #   adding the difference between the value of next state and the value of current state,\n",
    "    #   which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0)\n",
    "    def feedReward(self, reward):\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    # For when there's a new round\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    # After training, an agent has its policy stored in self.stateValues\n",
    "    # This can be saved to play against a human player\n",
    "    def savePolicy(self):\n",
    "        fw = open(currentdir + '/policies/c4_policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.statesValue, fw)\n",
    "        fw.close()\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.statesValue = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for human player\n",
    "# Mostly inherited from super class Player\n",
    "class HumanPlayer(Player):    \n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        while True:\n",
    "            try:\n",
    "                i = int(input(\"Input action column-> \"))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if (env.curRows[i], i) in positions:\n",
    "                return (env.curRows[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find vacant positions after a turn is made\n",
    "def availablePositions(board):\n",
    "    positions = []\n",
    "    for i in range(BOARD_COLS):\n",
    "        if env.curRows[i] > 0:\n",
    "            # Coordinates need to be in tuple form\n",
    "            positions.append((env.curRows[i] - 1,i))\n",
    "    return positions\n",
    "\n",
    "def startGame(p1, p2, env):\n",
    "    gameOver = False\n",
    "    human = p1.name == \"human\" or p2.name == \"human\"\n",
    "    observation = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "    while not gameOver:\n",
    "        # Player 1\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p1.chooseAction(openPositions, observation))\n",
    "        p1.addState(actionHash)\n",
    "        # When there's a human player, print out stuff\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            elif human:\n",
    "                print(\"tie!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        # Player 2\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p2.chooseAction(openPositions, observation))\n",
    "        p2.addState(actionHash)\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.]]\n",
      "saved p1 policy\n",
      "saved p2 policy\n"
     ]
    }
   ],
   "source": [
    "# Train agents\n",
    "p1 = AgentPlayer(\"p1\", 1)\n",
    "p2 = AgentPlayer(\"p2\", -1)\n",
    "env = gym.make('connect4env-v0')\n",
    "print(\"training...\")\n",
    "for _ in range(1):\n",
    "    startGame(p1, p2, env)\n",
    "# Save Results\n",
    "p1.savePolicy()\n",
    "print(\"saved p1 policy\")\n",
    "p2.savePolicy()\n",
    "print(\"saved p2 policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
