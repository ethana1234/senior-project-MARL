{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, numpy as np\n",
    "import connect4env\n",
    "\n",
    "BOARD_ROWS,BOARD_COLS = 6,7\n",
    "TOTAL_BOARD_SPACES = BOARD_ROWS*BOARD_COLS\n",
    "COORD_TO_INDEX = lambda x : (x[0] * BOARD_ROWS) + x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class\n",
    "\n",
    "Default super class that lays out defualt functions for both human and agent players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Default player class.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the player.\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        pass\n",
    "        \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning\n",
    "\n",
    "Agents of this class use a QLearning table to keep track of rewards.  Two of these agents will play against each other during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentPlayer:\n",
    "    '''\n",
    "    Agent player that learns from QLearning table.\n",
    "\n",
    "    Attributes:\n",
    "        name (str) : Name of Player.\n",
    "        turn (int) : Number that designates their turn.\n",
    "        explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        states (Python list) : All positions taken in current game\n",
    "        lr (float) : Learning Rate, used when feeding reward.\n",
    "        decayGamma (float) : Used when feeding reward.\n",
    "        statesValue (Python dict) : QLearning table. Stoes total reward for each possible game state.\n",
    "    '''\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        '''\n",
    "        Constructor for QAgentPlayer class.\n",
    "\n",
    "        Parameters:\n",
    "            name (str) : Name of Player.\n",
    "            turn (int) : Number that designates their turn.\n",
    "            explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        self.explRate = explRate\n",
    "        self.states = []\n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        self.statesValue = {}\n",
    "        \n",
    "    def getHash(self, board):\n",
    "        '''\n",
    "        Get a unique hash value that corresponds with the given board state.\n",
    "        \n",
    "        Parameters:\n",
    "            board (np.ndarray) : Current board (from environment).\n",
    "        \n",
    "        Returns:\n",
    "            str: Hash value of board.\n",
    "        '''\n",
    "        return str(board.reshape(BOARD_ROWS * BOARD_COLS))\n",
    "    \n",
    "    def addState(self, state):\n",
    "        '''\n",
    "        Adds a state to the Players state attribute\n",
    "        \n",
    "        Parameters:\n",
    "            state (str) : Put state's hash value into self.states after choosing action.\n",
    "        '''\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard):\n",
    "        '''\n",
    "        Choose an action to take using epsilon-greedy method.\n",
    "        \n",
    "        Parameters:\n",
    "            openPositions (Python list) : List of available places to make a move on board.\n",
    "            currentBoard (np.ndarray) : Game board.\n",
    "        \n",
    "        Returns:\n",
    "            int: Position of board that agent wants to make a move.\n",
    "        '''\n",
    "        if np.random.uniform(0,1) <= self.explRate:\n",
    "            # Take random action\n",
    "            index = np.random.choice(len(openPositions))\n",
    "            action = openPositions[index]\n",
    "        else:\n",
    "            maxValue = -999\n",
    "            for p in openPositions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                nextBoard[p] = self.turn\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        '''\n",
    "        At the end of the game, backpropogate and update state values.\n",
    "        The updated value of state t equals the current value of state t\n",
    "            adding the difference between the value of next state and the value of current state,\n",
    "            which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0).\n",
    "        \n",
    "        Parameters:\n",
    "            reward (float) : The reward determined by the environment.\n",
    "        '''\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset board when agent starts a new game.\n",
    "        '''\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        '''\n",
    "        After training, an agent has its policy stored in self.stateValues.\n",
    "        This function saves that attribute in a file to play later.\n",
    "        '''\n",
    "        with open(currentdir + '/policies/c4_policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.statesValue, fw)\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        '''\n",
    "        Reload previous self.stateValues.\n",
    "        \n",
    "        Parameters:\n",
    "            file (str) : Name of file that has policy.\n",
    "        '''\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.statesValue = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player\n",
    "\n",
    "Adds functionality for human player using raw input on the cmdline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    '''\n",
    "    Allows user to input moves through the cmdline.\n",
    "    \n",
    "    Parameters:\n",
    "        openPositions (Python list) : List of available places to make a move on board.\n",
    "        currentBoard (np.ndarray) : Game board.\n",
    "    \n",
    "    Returns:\n",
    "        int: Position of board that user wants to make a move.\n",
    "    '''\n",
    "    def chooseAction(self, positions, currentBoard=None):\n",
    "        while True:\n",
    "            try:\n",
    "                i = int(input(\"Input action column-> \"))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if (env.curRows[i], i) in positions:\n",
    "                return (env.curRows[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code\n",
    "\n",
    "The code below is how a game is started, Using 2 player objects and an instance of the tic tac toe environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def availablePositions(board):\n",
    "    '''\n",
    "    Find vacant positions after a turn is made\n",
    "    \n",
    "    Parameters:\n",
    "        board (np.ndarray) : Game board.\n",
    "    \n",
    "    Returns:\n",
    "        Python list: Vacant spaces on board.\n",
    "    '''\n",
    "    positions = []\n",
    "    for i in range(BOARD_COLS):\n",
    "        if env.curRows[i] > 0:\n",
    "            # Coordinates need to be in tuple form\n",
    "            positions.append((env.curRows[i] - 1,i))\n",
    "    return positions\n",
    "\n",
    "def startGame(p1, p2, env):\n",
    "    '''\n",
    "    Initiates a game of Connect 4.\n",
    "\n",
    "    Parameters:\n",
    "        p1 (Player) : Player who takes the first move.\n",
    "        p2 (Player) : Player who takes the second move.\n",
    "        env (TicTacToeEnv) : Environment for the game.\n",
    "        learn (bool) : Determines if agent players learn during the game.\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    ''' \n",
    "    gameOver = False\n",
    "    human = p1.name == \"human\" or p2.name == \"human\"\n",
    "    observation = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "    while not gameOver:\n",
    "        # Player 1\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p1.chooseAction(openPositions, observation))\n",
    "        p1.addState(actionHash)\n",
    "        # When there's a human player, print out stuff\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            elif human:\n",
    "                print(\"tie!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        # Player 2\n",
    "        openPositions = availablePositions(observation)\n",
    "        observation,reward,gameOver,actionHash = env.step(p2.chooseAction(openPositions, observation))\n",
    "        p2.addState(actionHash)\n",
    "        if human:\n",
    "            env.render()\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.]]\n",
      "saved p1 policy\n",
      "saved p2 policy\n"
     ]
    }
   ],
   "source": [
    "# Train agents\n",
    "p1 = AgentPlayer(\"p1\", 1)\n",
    "p2 = AgentPlayer(\"p2\", -1)\n",
    "env = gym.make('connect4env-v0')\n",
    "print(\"training...\")\n",
    "for _ in range(1):\n",
    "    startGame(p1, p2, env)\n",
    "# Save Results\n",
    "p1.savePolicy()\n",
    "print(\"saved p1 policy\")\n",
    "p2.savePolicy()\n",
    "print(\"saved p2 policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}