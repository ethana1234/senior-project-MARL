{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'backgammonenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c525f0208fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbackgammonenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTOT_BOARD_PTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mWHITE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBLACK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'backgammonenv'"
     ]
    }
   ],
   "source": [
    "import gym, pickle, numpy as np,random,copy,matplotlib.pyplot as plt\n",
    "import backgammonenv\n",
    "\n",
    "TOT_BOARD_PTS = 24\n",
    "WHITE,BLACK = 0,1\n",
    "ALL_CHECKERS = 15\n",
    "WHITE_HOME,BLACK_HOME,BAR_IND = 25,26,0\n",
    "roll_dice = lambda: (np.random.randint(1,7), np.random.randint(1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class\n",
    "\n",
    "Default super class that lays out defualt functions for both human and agent players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Default player class.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the player.\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard, dice, env):\n",
    "        pass\n",
    "        \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning\n",
    "\n",
    "Agents of this class use a QLearning table to keep track of rewards.  Two of these agents will play against each other during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentPlayer:\n",
    "    '''\n",
    "    Agent player that learns from QLearning table.\n",
    "\n",
    "    Attributes:\n",
    "        name (str) : Name of Player.\n",
    "        turn (int) : Number that designates their turn.\n",
    "        explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        states (Python list) : All positions taken in current game\n",
    "        lr (float) : Learning Rate, used when feeding reward.\n",
    "        decayGamma (float) : Used when feeding reward.\n",
    "        statesValue (Python dict) : QLearning table. Stoes total reward for each possible game state.\n",
    "    '''\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        '''\n",
    "        Constructor for QAgentPlayer class.\n",
    "\n",
    "        Parameters:\n",
    "            name (str) : Name of Player.\n",
    "            turn (int) : Number that designates their turn.\n",
    "            explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        self.explRate = explRate\n",
    "        self.states = []\n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        self.statesValue = {}\n",
    "        \n",
    "    def getHash(self, board):\n",
    "        '''\n",
    "        Get a unique hash value that corresponds with the given board state.\n",
    "        \n",
    "        Parameters:\n",
    "            board (np.ndarray) : Current board (from environment).\n",
    "        \n",
    "        Returns:\n",
    "            str: Hash value of board.\n",
    "        '''\n",
    "        return str(board.flatten)    \n",
    "\n",
    "    def addState(self, state):\n",
    "        '''\n",
    "        Adds a state to the Players state attribute\n",
    "        \n",
    "        Parameters:\n",
    "            state (str) : Put state's hash value into self.states after choosing action.\n",
    "        '''\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard, env, roll):\n",
    "        '''\n",
    "        Choose an action to take using epsilon-greedy method.\n",
    "        \n",
    "        Parameters:\n",
    "            openPositions (Python list) : List of available places to make a move on board.\n",
    "            currentBoard (np.ndarray) : Game board.\n",
    "        \n",
    "        Returns:\n",
    "            int: Position of board that agent wants to make a move.\n",
    "        '''\n",
    "        if not openPositions:\n",
    "            # No moves can be made\n",
    "            return []\n",
    "        \n",
    "        # Possible random action\n",
    "        index = np.random.choice(len(openPositions))\n",
    "        action = openPositions[index]\n",
    "        secondAction = action\n",
    "        maxValue = -999\n",
    "                    \n",
    "        if np.random.uniform(0,1) > self.explRate:\n",
    "            secondMax = -999\n",
    "            for p in openPositions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                for move in p:\n",
    "                    nextBoard = env.updateBoard(move,nextBoard)\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    secondMax = maxValue\n",
    "                    secondAction = action\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        if maxValue <= 0:\n",
    "            # If agent can't find a best move, pick a random one anyways\n",
    "            index = np.random.choice(len(openPositions))\n",
    "            action = openPositions[index]\n",
    "            secondAction = action\n",
    "        # 20% of the time actually choose the second best action, for variance in decision making\n",
    "        return action if np.random.uniform(0,1) > 0.2 else secondAction\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        '''\n",
    "        At the end of the game, backpropogate and update state values.\n",
    "        The updated value of state t equals the current value of state t\n",
    "            adding the difference between the value of next state and the value of current state,\n",
    "            which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0).\n",
    "        \n",
    "        Parameters:\n",
    "            reward (float) : The reward determined by the environment.\n",
    "        '''\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset board when agent starts a new game.\n",
    "        '''\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        '''\n",
    "        After training, an agent has its policy stored in self.stateValues.\n",
    "        This function saves that attribute in a file to play later.\n",
    "        '''\n",
    "        with open(currentdir + '/policies/c4_policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.statesValue, fw)\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        '''\n",
    "        Reload previous self.stateValues.\n",
    "        \n",
    "        Parameters:\n",
    "            file (str) : Name of file that has policy.\n",
    "        '''\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.statesValue = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player\n",
    "\n",
    "Adds functionality for human player using raw input on the cmdline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    '''\n",
    "    Allows user to input moves through the cmdline.\n",
    "    \n",
    "    Parameters:\n",
    "        openPositions (Python list) : List of available places to make a move on board.\n",
    "        currentBoard (np.ndarray) : Game board.\n",
    "    \n",
    "    Returns:\n",
    "        int: Position of board that user wants to make a move.\n",
    "    '''\n",
    "    def chooseAction(self, positions, currentBoard, env, roll):\n",
    "        message = f\"You rolled a {roll[0]} and {roll[1]}, select where you want to move checkers to and from\\n\"\n",
    "        while True:\n",
    "            try:\n",
    "                i = input(message+'First move (0 for bar, 25 for white home, 26 for black home: ')\n",
    "                i = tuple([int(x) for x in i.split(',')])\n",
    "                j = input(message+'Second move (0 for bar, 25 for white home, 26 for black home: ')\n",
    "                j = tuple([int(x) for x in j.split(',')])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if len(i) != 2 or len(j) != 2:\n",
    "                continue\n",
    "            elif (i,j) in positions:\n",
    "                return (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code\n",
    "\n",
    "The code below is how a game is started, Using 2 player objects and an instance of the tic tac toe environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startGame(pwhite, pblack, env):\n",
    "    '''\n",
    "    Initiates a game of Backgammon.\n",
    "\n",
    "    Parameters:\n",
    "        white (Player) : Player who uses white checkers.\n",
    "        black (Player) : Player who uses black checkers.\n",
    "        env (BackgammonEnv) : Environment for the game.        \n",
    "    ''' \n",
    "    gameOver = False\n",
    "    human = pwhite.name == \"human\" or pblack.name == \"human\"\n",
    "    # When there's a human player, print out stuff\n",
    "    if human:\n",
    "        env.render()\n",
    "\n",
    "    # Determine who goes first\n",
    "    roll = roll_dice()\n",
    "    while roll[0] == roll[1]:\n",
    "        # Reroll if they're equal\n",
    "        roll = roll_dice()\n",
    "\n",
    "    if roll[0] > roll[1]:\n",
    "        if human:\n",
    "            print('White goes first!')\n",
    "        env.playerTurn = WHITE\n",
    "        p1 = pwhite\n",
    "        p2 = pblack\n",
    "    else:\n",
    "        if human:\n",
    "            print('Black goes first!')\n",
    "        env.playerTurn = BLACK\n",
    "        p1 = pblack\n",
    "        p2 = pwhite\n",
    "\n",
    "    while not gameOver:\n",
    "        # Player who goes first\n",
    "        openPositions = env.availablePositions(roll) # openPositions returns a list of tuples, where each element in the tuple is an action, and the tuple represents an entire turn\n",
    "        # Example: [((4,6),(3,7)),((4,8),(2,4))]\n",
    "        #   In this example the dice roll came up 2 and 4. The first choice is to move a checker from spot 4 on the board to spot 6, and then move a checker from 3 to 7\n",
    "        #   The second choice is to move a checker from 4 to 8 and then 2 to 4\n",
    "        all_moves_in_turn = p1.chooseAction(openPositions, env.board, env, roll) # This will return a tuple with all the actions for the player's turn\n",
    "        for move in all_moves_in_turn:\n",
    "            # The for loop then loops through the tuple and invokes each action on the board\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p1.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = roll_dice()\n",
    "\n",
    "        # Player who goes second\n",
    "        openPositions = env.availablePositions(roll)\n",
    "        all_moves_in_turn = p2.chooseAction(openPositions, env.board, env, roll)\n",
    "        for move in all_moves_in_turn:\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p2.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = roll_dice()\n",
    "\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()\n",
    "    return 1 if reward[0] == 1 else (2 if reward[1] == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13|14|15|16|17|18|   |19|20|21|22|23|24\n",
      "--|--|--|--|--|--|---|--|--|--|--|--|--\n",
      "B |  |  |  |W |  |   |W |  |  |  |  |B \n",
      "B |  |  |  |W |  |   |W |  |  |  |  |B \n",
      "B |  |  |  |W |  |   |W |  |  |  |  |  \n",
      "B |  |  |  |  |  |   |W |  |  |  |  |  \n",
      "B |  |  |  |  |  |   |W |  |  |  |  |  \n",
      "--|--|--|--|--|--|   |--|--|--|--|--|--\n",
      "W |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |B |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |B |  |   |B |  |  |  |  |W \n",
      "W |  |  |  |B |  |   |B |  |  |  |  |W \n",
      "--|--|--|--|--|--|---|--|--|--|--|--|--\n",
      "12|11|10| 9| 8| 7|   | 6| 5| 4| 3| 2| 1\n",
      "Black goes first!\n",
      "13|14|15|16|17|18|   |19|20|21|22|23|24\n",
      "--|--|--|--|--|--|---|--|--|--|--|--|--\n",
      "B |  |  |  |W |  |   |W |  |  |  |  |B \n",
      "B |  |  |  |W |  |   |W |  |  |  |  |B \n",
      "B |  |  |  |W |  |   |W |  |  |  |  |  \n",
      "B |  |  |  |  |  |   |W |  |  |  |  |  \n",
      "  |  |  |  |  |  |   |W |  |  |  |  |  \n",
      "  |  |  |  |  |  |   |  |  |  |  |  |  \n",
      "--|--|--|--|--|--|   |--|--|--|--|--|--\n",
      "  |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |  |  |   |B |  |  |  |  |  \n",
      "W |  |  |  |B |  |   |B |  |  |  |  |W \n",
      "W |  |  |  |B |B |   |B |  |  |  |  |W \n",
      "--|--|--|--|--|--|---|--|--|--|--|--|--\n",
      "12|11|10| 9| 8| 7|   | 6| 5| 4| 3| 2| 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a50b2df4391d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'backgammonenv-v0'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#make env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstartGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHumanPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQAgentPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-eb1732e5f907>\u001b[0m in \u001b[0;36mstartGame\u001b[1;34m(pwhite, pblack, env)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Player who goes second\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mopenPositions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavailablePositions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mall_moves_in_turn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchooseAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopenPositions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_moves_in_turn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgameOver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactionHash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-45018f9a2fa1>\u001b[0m in \u001b[0;36mchooseAction\u001b[1;34m(self, positions, currentBoard, env, roll)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'First move (0 for bar, 25 for white home, 26 for black home: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'Second move (0 for bar, 25 for white home, 26 for black home: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "env = gym.make('backgammonenv-v0') #make env\n",
    "//startGame(HumanPlayer('human'),QAgentPlayer('p1',1),env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agents,numGames=2000,explRate=.3):\n",
    "    for agent in agents:\n",
    "        agent.lr = .2\n",
    "        agent.explRate = explRate\n",
    "    for i in range(numGames):\n",
    "        random.shuffle(agents)\n",
    "        agents[0].turn,agents[1].turn = 1,-1\n",
    "        startGame(agents[0],agents[1],env)\n",
    "        agents[2].turn,agents[3].turn = 1,-1\n",
    "        startGame(agents[2],agents[3],env)\n",
    "        agents[4].turn,agents[5].turn = 1,-1\n",
    "        startGame(agents[4],agents[5],env)\n",
    "\n",
    "def test(agents,numGames=2000):\n",
    "    for agent in agents:\n",
    "        agent.lr = 0\n",
    "        agent.explRate = .05\n",
    "\n",
    "    graphStats = {name:[0,0,0] for name in ['p1','p2','p3','p4','p5','p6']}\n",
    "\n",
    "    for i in range(numGames):\n",
    "        random.shuffle(agents)\n",
    "        agents[0].turn,agents[1].turn = 1,-1\n",
    "        result = startGame(agents[0],agents[1],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[0].name][2] += 1\n",
    "            graphStats[agents[1].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[0].name][0] += 1\n",
    "            graphStats[agents[1].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[0].name][1] += 1\n",
    "            graphStats[agents[1].name][0] += 1\n",
    "        agents[2].turn,agents[3].turn = 1,-1\n",
    "        result = startGame(agents[2],agents[3],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[2].name][2] += 1\n",
    "            graphStats[agents[3].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[2].name][0] += 1\n",
    "            graphStats[agents[3].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[2].name][1] += 1\n",
    "            graphStats[agents[3].name][0] += 1\n",
    "        agents[4].turn,agents[5].turn = 1,-1\n",
    "        result = startGame(agents[4],agents[5],env)\n",
    "        if result == 0:\n",
    "            graphStats[agents[4].name][2] += 1\n",
    "            graphStats[agents[5].name][2] += 1\n",
    "        elif result == 1:\n",
    "            graphStats[agents[4].name][0] += 1\n",
    "            graphStats[agents[5].name][1] += 1\n",
    "        elif result == 2:\n",
    "            graphStats[agents[4].name][1] += 1\n",
    "            graphStats[agents[5].name][0] += 1\n",
    "    return graphStats\n",
    "\n",
    "def graphResults(graphStats):\n",
    "    fig,axes = plt.subplots(2,3, figsize=(15,15))\n",
    "    xname = ['wins','losses','ties']\n",
    "    count = 0\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            agent = agents[count]\n",
    "            ax = axes[i,j]\n",
    "            ax.bar(xname,graphStats[agent.name])\n",
    "            ax.set_xticklabels(xname)\n",
    "            ax.set_title(f'{agent.name}\\'s Results')\n",
    "            count += 1\n",
    "    plt.show()\n",
    "\n",
    "def createNewGeneration(agents,graphStats):\n",
    "    maxScore = 0\n",
    "    for agent in agents:\n",
    "        agentScore = graphStats[agent.name][0] + (graphStats[agent.name][2] * .5)\n",
    "        if agentScore > maxScore:\n",
    "            # Agent with best score (accounting for wins and ties) will be the basis for the next generation\n",
    "            survivor = agent\n",
    "            maxScore = agentScore\n",
    "\n",
    "    np1 = copy.deepcopy(survivor)\n",
    "    np1.name = 'p1'\n",
    "    np2 = copy.deepcopy(survivor)\n",
    "    np2.name = 'p2'\n",
    "    np3 = copy.deepcopy(survivor)\n",
    "    np3.name = 'p3'\n",
    "    np4 = copy.deepcopy(survivor)\n",
    "    np4.name = 'p4'\n",
    "    np5 = copy.deepcopy(survivor)\n",
    "    np5.name = 'p5'\n",
    "    np6 = copy.deepcopy(survivor)\n",
    "    np6.name ='p6'\n",
    "    return [np1,np2,np3,np4,np5,np6]\n",
    "\n",
    "def savePolicy(agents):\n",
    "    for agent in agents:\n",
    "        agent.savePolicy()\n",
    "        print('saved an agent:',currentdir + '/policies/ttt_policy_' + str(agent.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = QAgentPlayer(\"p1\", 1)\n",
    "p2 = QAgentPlayer(\"p2\", -1)\n",
    "p3 = QAgentPlayer(\"p3\", 1)\n",
    "p4 = QAgentPlayer(\"p4\", -1)\n",
    "p5 = QAgentPlayer(\"p5\", 1)\n",
    "p6 = QAgentPlayer(\"p6\", -1)\n",
    "agents = [p1,p2,p3,p4,p5,p6]\n",
    "\n",
    "epochs = 10\n",
    "explore = .8\n",
    "for i in range(epochs):\n",
    "    train(agents,numGames=2000,explRate=explore)\n",
    "    explore -= .05\n",
    "    graphStats = test(agents)\n",
    "    agents = createNewGeneration(agents,graphStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
