{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, numpy as np\n",
    "import backgammonenv\n",
    "\n",
    "TOT_BOARD_PTS = 24\n",
    "WHITE,BLACK = 0,1\n",
    "ALL_CHECKERS = 15\n",
    "WHITE_HOME,BLACK_HOME,BAR_IND = 25,26,0\n",
    "roll_dice = lambda: return (np.random.randint(1,7), np.random.randint(1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "currentdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class\n",
    "\n",
    "Default super class that lays out defualt functions for both human and agent players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Default player class.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the player.\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard, dice, env):\n",
    "        pass\n",
    "        \n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning\n",
    "\n",
    "Agents of this class use a QLearning table to keep track of rewards.  Two of these agents will play against each other during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentPlayer:\n",
    "    '''\n",
    "    Agent player that learns from QLearning table.\n",
    "\n",
    "    Attributes:\n",
    "        name (str) : Name of Player.\n",
    "        turn (int) : Number that designates their turn.\n",
    "        explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        states (Python list) : All positions taken in current game\n",
    "        lr (float) : Learning Rate, used when feeding reward.\n",
    "        decayGamma (float) : Used when feeding reward.\n",
    "        statesValue (Python dict) : QLearning table. Stoes total reward for each possible game state.\n",
    "    '''\n",
    "    def __init__(self, name, turn, explRate=.3):\n",
    "        '''\n",
    "        Constructor for QAgentPlayer class.\n",
    "\n",
    "        Parameters:\n",
    "            name (str) : Name of Player.\n",
    "            turn (int) : Number that designates their turn.\n",
    "            explRate (float) : Percentage of time agent takes a random action vs greedy action when playing (default 30%).\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.turn = turn\n",
    "        self.explRate = explRate\n",
    "        self.states = []\n",
    "        self.lr = .2\n",
    "        self.decayGamma = .9\n",
    "        self.statesValue = {}\n",
    "        \n",
    "    def getHash(self, board):\n",
    "        '''\n",
    "        Get a unique hash value that corresponds with the given board state.\n",
    "        \n",
    "        Parameters:\n",
    "            board (np.ndarray) : Current board (from environment).\n",
    "        \n",
    "        Returns:\n",
    "            str: Hash value of board.\n",
    "        '''\n",
    "        return str(self.board.reshape(TOT_BOARD_PTS + 3))    \n",
    "\n",
    "    def addState(self, state):\n",
    "        '''\n",
    "        Adds a state to the Players state attribute\n",
    "        \n",
    "        Parameters:\n",
    "            state (str) : Put state's hash value into self.states after choosing action.\n",
    "        '''\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def chooseAction(self, openPositions, currentBoard, env, roll=None):\n",
    "        '''\n",
    "        Choose an action to take using epsilon-greedy method.\n",
    "        \n",
    "        Parameters:\n",
    "            openPositions (Python list) : List of available places to make a move on board.\n",
    "            currentBoard (np.ndarray) : Game board.\n",
    "        \n",
    "        Returns:\n",
    "            int: Position of board that agent wants to make a move.\n",
    "        '''\n",
    "        if not openPositions:\n",
    "            # No moves can be made\n",
    "            return []\n",
    "        if np.random.uniform(0,1) <= self.explRate:\n",
    "            # Take random action\n",
    "            index = np.random.choice(len(openPositions))\n",
    "            action = openPositions[index]\n",
    "        else:\n",
    "            maxValue = -999\n",
    "            for p in openPositions:\n",
    "                nextBoard = currentBoard.copy()\n",
    "                for move in p:\n",
    "                    nextBoard = env.updateBoard(move,nextBoard)\n",
    "                nextBoardHash = self.getHash(nextBoard)\n",
    "                value = 0 if self.statesValue.get(nextBoardHash) is None else self.statesValue.get(nextBoardHash)\n",
    "                if value > maxValue:\n",
    "                    maxValue = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        '''\n",
    "        At the end of the game, backpropogate and update state values.\n",
    "        The updated value of state t equals the current value of state t\n",
    "            adding the difference between the value of next state and the value of current state,\n",
    "            which is multiplied by a learning rate Î± (Given the reward of intermediate state is 0).\n",
    "        \n",
    "        Parameters:\n",
    "            reward (float) : The reward determined by the environment.\n",
    "        '''\n",
    "        for state in (reversed(self.states)):\n",
    "            if self.statesValue.get(state) is None:\n",
    "                self.statesValue[state] = 0\n",
    "            self.statesValue[state] += self.lr * (self.decayGamma * reward - self.statesValue[state])\n",
    "            reward = self.statesValue[state]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset board when agent starts a new game.\n",
    "        '''\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        '''\n",
    "        After training, an agent has its policy stored in self.stateValues.\n",
    "        This function saves that attribute in a file to play later.\n",
    "        '''\n",
    "        with open(currentdir + '/policies/c4_policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.statesValue, fw)\n",
    "\n",
    "    # Loading the policy when playing a human\n",
    "    def loadPolicy(self, file):\n",
    "        '''\n",
    "        Reload previous self.stateValues.\n",
    "        \n",
    "        Parameters:\n",
    "            file (str) : Name of file that has policy.\n",
    "        '''\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.statesValue = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player\n",
    "\n",
    "Adds functionality for human player using raw input on the cmdline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    '''\n",
    "    Allows user to input moves through the cmdline.\n",
    "    \n",
    "    Parameters:\n",
    "        openPositions (Python list) : List of available places to make a move on board.\n",
    "        currentBoard (np.ndarray) : Game board.\n",
    "    \n",
    "    Returns:\n",
    "        int: Position of board that user wants to make a move.\n",
    "    '''\n",
    "    def chooseAction(self, positions, currentBoard, env, ):\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "            except ValueError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code\n",
    "\n",
    "The code below is how a game is started, Using 2 player objects and an instance of the tic tac toe environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startGame(pwhite, pblack, env):\n",
    "    '''\n",
    "    Initiates a game of Backgammon.\n",
    "\n",
    "    Parameters:\n",
    "        white (Player) : Player who uses white checkers.\n",
    "        black (Player) : Player who uses black checkers.\n",
    "        env (BackgammonEnv) : Environment for the game.        \n",
    "    ''' \n",
    "    gameOver = False\n",
    "    human = p1.name == \"human\" or p2.name == \"human\"\n",
    "    # When there's a human player, print out stuff\n",
    "    if human:\n",
    "        env.render()\n",
    "\n",
    "    # Determine who goes first\n",
    "    roll = roll_dice()\n",
    "    while roll[0] == roll[1]:\n",
    "        # Reroll if they're equal\n",
    "        roll = roll_dice()\n",
    "\n",
    "    if roll[0] > roll[1]:\n",
    "        if human:\n",
    "            print('White goes first!')\n",
    "        env.playerTurn = WHITE\n",
    "        p1 = pwhite\n",
    "        p2 = pblack\n",
    "    else:\n",
    "        if human:\n",
    "            print('Black goes first!')\n",
    "        env.playerTurn = BLACK\n",
    "        p1 = pblack\n",
    "        p2 = pwhite\n",
    "\n",
    "    while not gameOver:\n",
    "        # Player who goes first\n",
    "        openPositions = env.availablePositions(roll) # openPositions returns a list of tuples, where each element in the tuple is an action, and the tuple represents an entire turn\n",
    "        # Example: [((4,6),(3,7)),((4,8),(2,4))]\n",
    "        #   In this example the dice roll came up 2 and 4. The first choice is to move a checker from spot 4 on the board to spot 6, and then move a checker from 3 to 7\n",
    "        #   The second choice is to move a checker from 4 to 8 and then 2 to 4\n",
    "        all_moves_in_turn = p1.chooseAction(openPositions, env.board, env, roll) # This will return a tuple with all the actions for the player's turn\n",
    "        for move in all_moves_in_turn:\n",
    "            # The for loop then loops through the tuple and invokes each action on the board\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p1.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human and reward[0] == 1:\n",
    "                print(f\"{p1.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = make_roll()\n",
    "\n",
    "        # Player who goes second\n",
    "        openPositions = env.availablePositions(roll)\n",
    "        all_moves_in_turn = p2.chooseAction(openPositions, env.board, env, roll)\n",
    "        for move in all_moves_in_turn:\n",
    "            reward,gameOver,actionHash = env.step(move)\n",
    "            if gameOver:\n",
    "                break\n",
    "            p2.addState(actionHash)\n",
    "        if human:\n",
    "            if all_moves_in_turn:\n",
    "                env.render()\n",
    "            else:\n",
    "                print('No moves can be made')\n",
    "        if gameOver:\n",
    "            if human:\n",
    "                print(f\"{p2.name} wins!\")\n",
    "            p1.feedReward(reward[0])\n",
    "            p2.feedReward(reward[1])\n",
    "            break\n",
    "        roll = make_roll()\n",
    "\n",
    "    env.reset()\n",
    "    p1.reset()\n",
    "    p2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.]]\n",
      "saved p1 policy\n",
      "saved p2 policy\n"
     ]
    }
   ],
   "source": [
    "# Train agents\n",
    "p1 = AgentPlayer(\"p1\", WHITE)\n",
    "p2 = AgentPlayer(\"p2\", BLACK)\n",
    "env = gym.make('backgammonenv-v0')\n",
    "print(\"training...\")\n",
    "for _ in range(1):\n",
    "    startGame(p1, p2, env)\n",
    "# Save Results\n",
    "p1.savePolicy()\n",
    "print(\"saved p1 policy\")\n",
    "p2.savePolicy()\n",
    "print(\"saved p2 policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}